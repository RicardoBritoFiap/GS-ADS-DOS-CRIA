{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install scikit-learn==1.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_fit_context' from 'sklearn.base' (c:\\Users\\pytho\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\pytho\\Documents\\GitHub\\GS-ADS-DOS-CRIA\\AI & CHATBOT\\Hemograma\\Model\\Modelo.ipynb Célula 1\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pytho/Documents/GitHub/GS-ADS-DOS-CRIA/AI%20%26%20CHATBOT/Hemograma/Model/Modelo.ipynb#W0sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m cross_val_score, GridSearchCV, train_test_split, KFold\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pytho/Documents/GitHub/GS-ADS-DOS-CRIA/AI%20%26%20CHATBOT/Hemograma/Model/Modelo.ipynb#W0sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m confusion_matrix, recall_score, precision_score, precision_recall_curve\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/pytho/Documents/GitHub/GS-ADS-DOS-CRIA/AI%20%26%20CHATBOT/Hemograma/Model/Modelo.ipynb#W0sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mensemble\u001b[39;00m \u001b[39mimport\u001b[39;00m RandomForestClassifier\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pytho/Documents/GitHub/GS-ADS-DOS-CRIA/AI%20%26%20CHATBOT/Hemograma/Model/Modelo.ipynb#W0sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdummy\u001b[39;00m \u001b[39mimport\u001b[39;00m DummyClassifier\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/pytho/Documents/GitHub/GS-ADS-DOS-CRIA/AI%20%26%20CHATBOT/Hemograma/Model/Modelo.ipynb#W0sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mimblearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mover_sampling\u001b[39;00m \u001b[39mimport\u001b[39;00m SMOTE\n",
      "File \u001b[1;32mc:\\Users\\pytho\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\ensemble\\__init__.py:5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mThe :mod:`sklearn.ensemble` module includes ensemble-based methods for\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mclassification, regression and anomaly detection.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_bagging\u001b[39;00m \u001b[39mimport\u001b[39;00m BaggingClassifier, BaggingRegressor\n\u001b[0;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_base\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseEnsemble\n\u001b[0;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_forest\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m      8\u001b[0m     ExtraTreesClassifier,\n\u001b[0;32m      9\u001b[0m     ExtraTreesRegressor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m     RandomTreesEmbedding,\n\u001b[0;32m     13\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\pytho\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\ensemble\\_bagging.py:16\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mwarnings\u001b[39;00m \u001b[39mimport\u001b[39;00m warn\n\u001b[0;32m     14\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m ClassifierMixin, RegressorMixin, _fit_context\n\u001b[0;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m accuracy_score, r2_score\n\u001b[0;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtree\u001b[39;00m \u001b[39mimport\u001b[39;00m DecisionTreeClassifier, DecisionTreeRegressor\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name '_fit_context' from 'sklearn.base' (c:\\Users\\pytho\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py)"
     ]
    }
   ],
   "source": [
    "#━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━❮Bibliotecas❯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━❮◆❯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "from scipy.stats import ks_2samp\n",
    "from collections import OrderedDict\n",
    "from operator import itemgetter\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split, KFold\n",
    "from sklearn.metrics import confusion_matrix, recall_score, precision_score, precision_recall_curve\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from joblib import dump\n",
    "#━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━❮◆❯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ler o arquivo Excel\n",
    "df = pd.read_excel('/content/dataset.xlsx')\n",
    "\n",
    "# Filtrar casos positivos e negativos\n",
    "df_positive = df[df['SARS-Cov-2 exam result'] == 'positive']\n",
    "df_negative = df[df['SARS-Cov-2 exam result'] == 'negative']\n",
    "\n",
    "# Remover colunas específicas\n",
    "df = df.drop(['Patient addmited to regular ward (1=yes, 0=no)',\n",
    "              'Patient addmited to semi-intensive unit (1=yes, 0=no)',\n",
    "              'Patient addmited to intensive care unit (1=yes, 0=no)'], axis=1)\n",
    "\n",
    "# Definir 'Patient ID' como índice do DataFrame\n",
    "df = df.set_index('Patient ID')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_analyze = df.isna().sum()/len(df)\n",
    "\n",
    "print(\"Quantidade de tuplas:\", len(df))\n",
    "print(\"Percentual médio missing values:\", round(nan_analyze.mean()*100,1),\"%\")\n",
    "\n",
    "label_perc = []\n",
    "for i in np.arange(0, len(df.columns), 10):\n",
    "    label_perc.append(str(i)+\"%\")\n",
    "plt.figure(figsize=[10,40])\n",
    "\n",
    "plt.yticks(np.arange(len(df.columns)), nan_analyze.index.values)\n",
    "plt.xticks(np.arange(0, 1.1, .1), label_perc)\n",
    "\n",
    "plt.ylim(0,len(df.columns))\n",
    "\n",
    "plt.barh(np.arange(len(df.columns)), nan_analyze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_filtered[nan_analyze_filtered[nan_analyze_filtered<=.4].index.values]\n",
    "\n",
    "nan_analyze_filtered = df_filtered.isna().sum()/len(df_filtered)\n",
    "\n",
    "print(\"Quantidade de tuplas:\", len(df_filtered))\n",
    "print(\"Percentual médio missing values:\", round(nan_analyze_filtered.mean()*100,1),\"%\")\n",
    "\n",
    "label_perc = []\n",
    "for i in np.arange(0, 110, 10):\n",
    "    label_perc.append(str(i)+\"%\")\n",
    "plt.figure(figsize=[10,10])\n",
    "\n",
    "plt.yticks(np.arange(len(df_filtered.columns)), nan_analyze_filtered.index.values)\n",
    "plt.xticks(np.arange(0, 1.1, .1), label_perc)\n",
    "\n",
    "plt.ylim(0,len(df_filtered.columns))\n",
    "\n",
    "plt.barh(np.arange(len(df_filtered.columns)), nan_analyze_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_filtered.dtypes[df_filtered.dtypes=='float64'].index.values\n",
    "\n",
    "ks_list = []\n",
    "pvalue_list = []\n",
    "feature_list = []\n",
    "\n",
    "for feature in features:\n",
    "    \n",
    "    positive = df_positive[~np.isnan(df_positive[feature])]\n",
    "    negative = df_negative[~np.isnan(df_negative[feature])]\n",
    "    \n",
    "    if len(positive)*len(negative)>0:\n",
    "        ks, pvalue = ks_2samp(positive[feature], negative[feature])\n",
    "        ks_list.append(ks)\n",
    "        pvalue_list.append(pvalue)\n",
    "        feature_list.append(feature)\n",
    "        \n",
    "df_ks = pd.DataFrame(data=zip(ks_list,pvalue_list),columns=['ks', 'pvalue'],index=feature_list)\n",
    "df_ks = df_ks.sort_values(by='ks',ascending=True)\n",
    "\n",
    "df_ks['ks']\n",
    "plt.figure(figsize=(8,15))\n",
    "plt.yticks(np.arange(len(df_ks)), df_ks.index.values)\n",
    "plt.title('Diferença entre positivo VS negativo')\n",
    "plt.barh(np.arange(len(df_ks)), df_ks['ks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_treated = df_filtered\n",
    "cat_features = df_filtered.dtypes[df_filtered.dtypes == 'object'].index.values\n",
    "\n",
    "for feature in cat_features:\n",
    "    df_treated[feature] = df_treated[feature].fillna(df_treated[feature].mode().values[0]) \n",
    "    \n",
    "df_treated = df_treated.fillna(df_treated.median())\n",
    "\n",
    "df_treated_dummies = pd.get_dummies(df_treated, drop_first=True, dtype='bool')\n",
    "\n",
    "columns = list(df_treated_dummies.drop(labels=['SARS-Cov-2 exam result_positive'],axis=1).columns.values)\n",
    "columns.append('SARS-Cov-2 exam result_positive')\n",
    "\n",
    "df_treated_dummies = df_treated_dummies[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_treated['SARS-Cov-2 exam result'].value_counts().plot(kind='bar',\n",
    "                                    figsize=(14,8))\n",
    "ax.set_xticklabels(['negative', 'positive'], rotation=0, fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Supondo que df_treated_dummies é o seu DataFrame\n",
    "y = df_treated_dummies['SARS-Cov-2 exam result_positive']\n",
    "x = df_treated_dummies.iloc[:,:-1]\n",
    "x['Random'] = np.random.rand(x.shape[0])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=42)\n",
    "\n",
    "smote = SMOTE()\n",
    "# Use fit_resample em vez de fit_sample\n",
    "X_train_random, y_train_random = smote.fit_resample(X_train, y_train)\n",
    "X_train, y_train = smote.fit_resample(X_train.iloc[:, :-1], y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=20)\n",
    "kfold = KFold(n_splits=20, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split':[2, 4, 6],\n",
    "    'min_samples_leaf':[2, 4, 6],\n",
    "    'n_estimators':[10, 30, 50],\n",
    "    'max_depth':[3, 5]\n",
    "    }\n",
    "\n",
    "clf_rf = RandomForestClassifier(random_state=42)\n",
    "grid = GridSearchCV(estimator=clf_rf, param_grid=param_grid, cv=kfold, scoring='recall', n_jobs=-1)\n",
    "grid.fit(X=X_train, y=y_train)\n",
    "clf_rf = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_rf.fit(X_train_random, y_train_random)\n",
    "    \n",
    "cols = x.columns\n",
    "feature_importance = pd.DataFrame(data=clf_rf.feature_importances_, index=cols, columns=['FI'])\n",
    "feature_importance = feature_importance.sort_values(by='FI', ascending=True)\n",
    "    \n",
    "plt.figure(figsize=(8,10))\n",
    "\n",
    "plt.yticks(np.arange(len(feature_importance)), feature_importance.index.values)\n",
    "plt.barh(np.arange(len(feature_importance)), feature_importance['FI'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se necessário, adicione a coluna 'Random' a X_train\n",
    "X_train['Random'] = np.random.rand(X_train.shape[0])\n",
    "\n",
    "# Treinar o modelo com X_train que inclui a coluna 'Random'\n",
    "clf_rf.fit(X_train, y_train)\n",
    "\n",
    "# Adicionar a coluna 'Random' a X_test\n",
    "X_test['Random'] = np.random.rand(X_test.shape[0])\n",
    "\n",
    "# Gerar a matriz de confusão\n",
    "cm = confusion_matrix(y_test, clf_rf.predict(X_test))\n",
    "\n",
    "# Plotar a matriz de confusão\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='g', cmap='Blues')\n",
    "plt.title('Matriz de Confusão')\n",
    "plt.xlabel('Previsto')\n",
    "plt.ylabel('Verdadeiro')\n",
    "plt.show()\n",
    "\n",
    "# Calculando e imprimindo o recall\n",
    "print(\"Recall treino:\", recall_score(y_train, clf_rf.predict(X_train)))\n",
    "print(\"Recall validação:\", cross_val_score(clf_rf, X_train, y_train, cv=20, scoring='recall', n_jobs=-1).mean())\n",
    "print(\"Recall teste:\", recall_score(y_test, clf_rf.predict(X_test)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = pd.DataFrame(data=clf_rf.feature_importances_, index=cols, columns=['FI'])\n",
    "feature_importance = feature_importance.sort_values(by='FI', ascending=True)\n",
    "\n",
    "\n",
    "# Selecionando as 5 características mais importantes\n",
    "important_features = feature_importance.sort_values(by='FI', ascending=False).head(5).index.tolist()\n",
    "print(\"Top 5 características importantes:\", important_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suponha que X_train é o DataFrame que foi usado para treinar o modelo\n",
    "all_features = X_train.columns.tolist()\n",
    "print(\"Todas as características utilizadas para treinar o modelo:\")\n",
    "print(all_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suponha que X_train é o DataFrame que foi usado para treinar o modelo\n",
    "all_features = X_train.columns.tolist()\n",
    "print(\"Todas as características utilizadas para treinar o modelo:\")\n",
    "print(all_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(clf_rf, 'RandomForest.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def make_prediction(input_features, model, all_features, important_features):\n",
    "    \"\"\"\n",
    "    Faz uma previsão com base nas entradas do usuário.\n",
    "\n",
    "    :param input_features: Um dicionário com as características importantes e seus valores.\n",
    "    :param model: O modelo de classificação treinado para fazer a previsão.\n",
    "    :param all_features: Lista de todas as características usadas para treinar o modelo.\n",
    "    :param important_features: Lista das características importantes que o usuário deve fornecer.\n",
    "    :return: A previsão do modelo.\n",
    "    \"\"\"\n",
    "\n",
    "    # Verificar se todas as características importantes foram fornecidas\n",
    "    for feature in important_features:\n",
    "        if feature not in input_features:\n",
    "            raise ValueError(f\"Faltando valor para a característica importante: {feature}\")\n",
    "\n",
    "    # Converter as entradas do usuário em um DataFrame\n",
    "    user_input_df = pd.DataFrame([input_features])\n",
    "\n",
    "    # Adicionar colunas faltantes e organizar as colunas na mesma ordem do treinamento\n",
    "    for feature in all_features:\n",
    "        if feature not in user_input_df.columns:\n",
    "            user_input_df[feature] = 0  # Preencher com um valor padrão\n",
    "    user_input_df = user_input_df[all_features]\n",
    "\n",
    "    # Fazer a previsão\n",
    "    prediction = model.predict(user_input_df)\n",
    "\n",
    "    return prediction\n",
    "\n",
    "# Exemplo de uso da função:\n",
    "# Suponha que clf_rf seja o seu modelo treinado, all_features a lista completa de características, \n",
    "# e important_features as 5 características principais.\n",
    "# Você chamaria a função assim:\n",
    "# result = make_prediction(input_features={'Feature1': valor1, 'Feature2': valor2, ...}, \n",
    "#                          model=clf_rf, \n",
    "#                          all_features=all_features, \n",
    "#                          important_features=important_features)\n",
    "# print(\"Previsão do modelo:\", result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
